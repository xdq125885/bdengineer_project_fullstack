"""
æ¼”ç¤ºè„šæœ¬ - å±•ç¤ºå¦‚ä½•ä½¿ç”¨è‡ªåŠ¨åŒ–æµ‹è¯„ç³»ç»Ÿ
åŸºäºç”¨æˆ·ç™»å½•çš„PRDã€AIç”Ÿæˆç”¨ä¾‹ã€äººå·¥ç¼–å†™ç”¨ä¾‹è¿›è¡Œè¯„æµ‹
"""

import json
import logging
from pathlib import Path

from evaluation.evaluator import Evaluator
from evaluation.visualizer import Visualizer
from evaluation.utils import FileUtils, Logger, ReportGenerator, TestCaseParser
from evaluation.config import EVALUATION_RESULTS_DIR, LOG_DIR

# è®¾ç½®æ—¥å¿—
logger = Logger.setup_logger(
    "demo",
    log_file=str(LOG_DIR / "demo.log"),
    level="INFO"
)


def load_json_cases(json_file: str) -> list:
    """
    ä»JSONæ–‡ä»¶åŠ è½½æµ‹è¯•ç”¨ä¾‹
    
    Args:
        json_file: JSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ï¼ˆè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼‰
    """
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            cases = json.load(f)
        
        # å°†JSONæ ¼å¼çš„ç”¨ä¾‹è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
        text_cases = []
        for case in cases:
            case_text = format_case_from_json(case)
            text_cases.append(case_text)
        
        logger.info(f"ä» {json_file} åŠ è½½äº† {len(text_cases)} ä¸ªç”¨ä¾‹")
        return text_cases
    
    except Exception as e:
        logger.error(f"åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {e}")
        return []


def format_case_from_json(case: dict) -> str:
    """
    å°†JSONæ ¼å¼çš„ç”¨ä¾‹è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
    
    Args:
        case: JSONæ ¼å¼çš„ç”¨ä¾‹å­—å…¸
        
    Returns:
        æ–‡æœ¬æ ¼å¼çš„ç”¨ä¾‹
    """
    lines = []
    
    # æ ‡é¢˜
    if case.get("title"):
        lines.append(f"# {case['title']}")
        lines.append("")
    
    # å‰ç½®æ¡ä»¶
    if case.get("preconditions"):
        lines.append("## å‰ç½®æ¡ä»¶")
        preconditions = case["preconditions"]
        if isinstance(preconditions, list):
            for pre in preconditions:
                lines.append(f"- {pre}")
        else:
            lines.append(f"- {preconditions}")
        lines.append("")
    
    # æ“ä½œæ­¥éª¤
    if case.get("steps"):
        lines.append("## æ“ä½œæ­¥éª¤")
        steps = case["steps"]
        if isinstance(steps, list):
            for step in steps:
                lines.append(f"- {step}")
        else:
            lines.append(f"- {steps}")
        lines.append("")
    
    # é¢„æœŸç»“æœ
    if case.get("expected"):
        lines.append("## é¢„æœŸç»“æœ")
        expected = case["expected"]
        if isinstance(expected, list):
            for exp in expected:
                lines.append(f"- {exp}")
        else:
            lines.append(f"- {expected}")
        lines.append("")
    
    return "\n".join(lines)


def run_demo():
    """è¿è¡Œæ¼”ç¤º"""
    
    logger.info("=" * 80)
    logger.info("è‡ªåŠ¨åŒ–æµ‹è¯„ç³»ç»Ÿæ¼”ç¤º - ç”¨æˆ·ç™»å½•åŠŸèƒ½")
    logger.info("=" * 80)
    logger.info("")
    
    # æ–‡ä»¶è·¯å¾„
    prd_file = "ç”¨æˆ·ç™»å½•.md"
    ai_cases_file = "PRDAI1.json"
    human_cases_file = "prdrengong.json"
    output_dir = str(EVALUATION_RESULTS_DIR / "demo_results")
    
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # 1. åŠ è½½æ•°æ®
    logger.info("ã€æ­¥éª¤1ã€‘åŠ è½½æ•°æ®")
    logger.info("-" * 80)
    
    # è¯»å–PRD
    logger.info(f"è¯»å–PRDæ–‡ä»¶: {prd_file}")
    prd_text = FileUtils.read_text(prd_file)
    logger.info(f"PRDé•¿åº¦: {len(prd_text)} å­—ç¬¦")
    logger.info("")
    
    # åŠ è½½AIç”Ÿæˆçš„ç”¨ä¾‹
    logger.info(f"åŠ è½½AIç”Ÿæˆçš„ç”¨ä¾‹: {ai_cases_file}")
    ai_cases = load_json_cases(ai_cases_file)
    logger.info(f"AIç”Ÿæˆç”¨ä¾‹æ•°: {len(ai_cases)}")
    logger.info("")
    
    # åŠ è½½äººå·¥ç¼–å†™çš„ç”¨ä¾‹
    logger.info(f"åŠ è½½äººå·¥ç¼–å†™çš„ç”¨ä¾‹: {human_cases_file}")
    human_cases = load_json_cases(human_cases_file)
    logger.info(f"äººå·¥ç¼–å†™ç”¨ä¾‹æ•°: {len(human_cases)}")
    logger.info("")
    
    # 2. åˆå§‹åŒ–è¯„æµ‹å™¨
    logger.info("ã€æ­¥éª¤2ã€‘åˆå§‹åŒ–è¯„æµ‹å™¨")
    logger.info("-" * 80)
    logger.info("åˆå§‹åŒ–è¯„æµ‹å™¨ï¼ˆåŒ…æ‹¬ç›¸ä¼¼åº¦æ¨¡å‹ï¼‰...")
    evaluator = Evaluator(use_similarity_model=True)
    logger.info("è¯„æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
    logger.info("")
    
    # 3. è¯„æµ‹AIç”Ÿæˆçš„ç”¨ä¾‹
    logger.info("ã€æ­¥éª¤3ã€‘è¯„æµ‹AIç”Ÿæˆçš„ç”¨ä¾‹")
    logger.info("-" * 80)
    logger.info("æ‰§è¡Œè¯„æµ‹...")
    ai_eval_results = evaluator.evaluate_batch(
        ai_cases,
        reference_cases=human_cases,
        prd_text=prd_text
    )
    logger.info("AIç”¨ä¾‹è¯„æµ‹å®Œæˆ")
    logger.info("")
    
    # 4. è¯„æµ‹äººå·¥ç¼–å†™çš„ç”¨ä¾‹
    logger.info("ã€æ­¥éª¤4ã€‘è¯„æµ‹äººå·¥ç¼–å†™çš„ç”¨ä¾‹")
    logger.info("-" * 80)
    logger.info("æ‰§è¡Œè¯„æµ‹...")
    human_eval_results = evaluator.evaluate_batch(
        human_cases,
        reference_cases=None,  # äººå·¥ç”¨ä¾‹ä½œä¸ºå‚è€ƒï¼Œä¸éœ€è¦å¯¹æ¯”
        prd_text=prd_text
    )
    logger.info("äººå·¥ç”¨ä¾‹è¯„æµ‹å®Œæˆ")
    logger.info("")
    
    # 5. ç‰ˆæœ¬å¯¹æ¯”
    logger.info("ã€æ­¥éª¤5ã€‘ç‰ˆæœ¬å¯¹æ¯”åˆ†æ")
    logger.info("-" * 80)
    logger.info("å¯¹æ¯”AIç”Ÿæˆç”¨ä¾‹ä¸äººå·¥ç¼–å†™ç”¨ä¾‹...")
    comparison_results = evaluator.compare_versions(
        ai_cases,
        human_cases,
        reference_cases=None,
        prd_text=prd_text
    )
    logger.info("ç‰ˆæœ¬å¯¹æ¯”å®Œæˆ")
    logger.info("")
    
    # 6. ç”ŸæˆæŠ¥å‘Š
    logger.info("ã€æ­¥éª¤6ã€‘ç”ŸæˆæŠ¥å‘Š")
    logger.info("-" * 80)
    
    # AIç”¨ä¾‹è¯„æµ‹æŠ¥å‘Š
    ai_report_file = Path(output_dir) / "ai_evaluation_report.json"
    FileUtils.write_json(ai_eval_results, str(ai_report_file))
    logger.info(f"âœ“ AIç”¨ä¾‹è¯„æµ‹æŠ¥å‘Š: {ai_report_file}")
    
    # äººå·¥ç”¨ä¾‹è¯„æµ‹æŠ¥å‘Š
    human_report_file = Path(output_dir) / "human_evaluation_report.json"
    FileUtils.write_json(human_eval_results, str(human_report_file))
    logger.info(f"âœ“ äººå·¥ç”¨ä¾‹è¯„æµ‹æŠ¥å‘Š: {human_report_file}")
    
    # ç‰ˆæœ¬å¯¹æ¯”æŠ¥å‘Š
    comparison_file = Path(output_dir) / "version_comparison.json"
    FileUtils.write_json(comparison_results, str(comparison_file))
    logger.info(f"âœ“ ç‰ˆæœ¬å¯¹æ¯”æŠ¥å‘Š: {comparison_file}")
    
    # ç”ŸæˆHTMLæŠ¥å‘Š
    visualizer = Visualizer(output_dir)
    
    ai_html_file = Path(output_dir) / "ai_evaluation_report.html"
    visualizer.export_results(ai_eval_results, str(ai_html_file), format="html")
    logger.info(f"âœ“ AIç”¨ä¾‹HTMLæŠ¥å‘Š: {ai_html_file}")
    
    human_html_file = Path(output_dir) / "human_evaluation_report.html"
    visualizer.export_results(human_eval_results, str(human_html_file), format="html")
    logger.info(f"âœ“ äººå·¥ç”¨ä¾‹HTMLæŠ¥å‘Š: {human_html_file}")
    
    logger.info("")
    
    # 7. æ‰“å°å¯¹æ¯”æ‘˜è¦
    logger.info("ã€æ­¥éª¤7ã€‘å¯¹æ¯”æ‘˜è¦")
    logger.info("-" * 80)
    
    print_comparison_summary(ai_eval_results, human_eval_results, comparison_results)
    
    logger.info("")
    logger.info("=" * 80)
    logger.info("æ¼”ç¤ºå®Œæˆï¼æ‰€æœ‰æŠ¥å‘Šå·²ç”Ÿæˆ")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    logger.info("=" * 80)


def print_comparison_summary(ai_results: dict, human_results: dict, comparison: dict):
    """
    æ‰“å°å¯¹æ¯”æ‘˜è¦
    
    Args:
        ai_results: AIç”¨ä¾‹è¯„æµ‹ç»“æœ
        human_results: äººå·¥ç”¨ä¾‹è¯„æµ‹ç»“æœ
        comparison: ç‰ˆæœ¬å¯¹æ¯”ç»“æœ
    """
    
    # AIç”¨ä¾‹åˆ†æ•°
    ai_scores = ai_results.get("aggregate_scores", {})
    ai_overall = ai_results.get("overall_score", 0)
    
    # äººå·¥ç”¨ä¾‹åˆ†æ•°
    human_scores = human_results.get("aggregate_scores", {})
    human_overall = human_results.get("overall_score", 0)
    
    logger.info("")
    logger.info("ğŸ“Š ã€ç»¼åˆåˆ†æ•°å¯¹æ¯”ã€‘")
    logger.info(f"  AIç”Ÿæˆç”¨ä¾‹ç»¼åˆåˆ†æ•°:    {ai_overall:.4f}")
    logger.info(f"  äººå·¥ç¼–å†™ç”¨ä¾‹ç»¼åˆåˆ†æ•°:  {human_overall:.4f}")
    logger.info(f"  å·®å¼‚:                 {abs(ai_overall - human_overall):.4f}")
    logger.info("")
    
    logger.info("ğŸ“ˆ ã€å„ç»´åº¦åˆ†æ•°å¯¹æ¯”ã€‘")
    logger.info(f"{'æŒ‡æ ‡':<20} {'AIç”Ÿæˆ':<15} {'äººå·¥ç¼–å†™':<15} {'å·®å¼‚':<15}")
    logger.info("-" * 65)
    
    metrics = [
        ("ç»“æ„å®Œæ•´æ€§", "avg_structure_score"),
        ("å†…å®¹è´¨é‡", "avg_quality_score"),
        ("å»é‡æ€§", "uniqueness_score"),
        ("è¦†ç›–ç‡", "coverage_score"),
        ("ç›¸ä¼¼åº¦", "similarity_score"),
    ]
    
    for metric_name, metric_key in metrics:
        ai_score = ai_scores.get(metric_key, 0)
        human_score = human_scores.get(metric_key, 0)
        diff = ai_score - human_score
        
        logger.info(f"{metric_name:<20} {ai_score:<15.4f} {human_score:<15.4f} {diff:+.4f}")
    
    logger.info("")
    
    # è¯¦ç»†åˆ†æ
    logger.info("ğŸ” ã€è¯¦ç»†åˆ†æã€‘")
    
    # å»é‡æ€§åˆ†æ
    if "uniqueness" in ai_results.get("detailed_analysis", {}):
        ai_unique = ai_results["detailed_analysis"]["uniqueness"]
        logger.info("")
        logger.info("  AIç”Ÿæˆç”¨ä¾‹å»é‡æ€§:")
        logger.info(f"    - å®Œå…¨é‡å¤: {ai_unique.get('exact_duplicate_count', 0)}")
        logger.info(f"    - é«˜åº¦ç›¸ä¼¼: {ai_unique.get('near_duplicate_count', 0)}")
        logger.info(f"    - å¤šæ ·æ€§åˆ†æ•°: {ai_unique.get('diversity_score', 0):.4f}")
    
    if "uniqueness" in human_results.get("detailed_analysis", {}):
        human_unique = human_results["detailed_analysis"]["uniqueness"]
        logger.info("")
        logger.info("  äººå·¥ç¼–å†™ç”¨ä¾‹å»é‡æ€§:")
        logger.info(f"    - å®Œå…¨é‡å¤: {human_unique.get('exact_duplicate_count', 0)}")
        logger.info(f"    - é«˜åº¦ç›¸ä¼¼: {human_unique.get('near_duplicate_count', 0)}")
        logger.info(f"    - å¤šæ ·æ€§åˆ†æ•°: {human_unique.get('diversity_score', 0):.4f}")
    
    # è¦†ç›–ç‡åˆ†æ
    if "coverage" in ai_results.get("detailed_analysis", {}):
        ai_coverage = ai_results["detailed_analysis"]["coverage"]
        logger.info("")
        logger.info("  AIç”Ÿæˆç”¨ä¾‹è¦†ç›–ç‡:")
        logger.info(f"    - éœ€æ±‚è¦†ç›–: {ai_coverage['requirement_coverage'].get('coverage_rate', 0):.4f}")
        logger.info(f"    - åŠŸèƒ½è¦†ç›–: {ai_coverage['feature_coverage'].get('feature_coverage_rate', 0):.4f}")
        logger.info(f"    - ç»¼åˆè¦†ç›–: {ai_coverage.get('overall_coverage', 0):.4f}")
    
    if "coverage" in human_results.get("detailed_analysis", {}):
        human_coverage = human_results["detailed_analysis"]["coverage"]
        logger.info("")
        logger.info("  äººå·¥ç¼–å†™ç”¨ä¾‹è¦†ç›–ç‡:")
        logger.info(f"    - éœ€æ±‚è¦†ç›–: {human_coverage['requirement_coverage'].get('coverage_rate', 0):.4f}")
        logger.info(f"    - åŠŸèƒ½è¦†ç›–: {human_coverage['feature_coverage'].get('feature_coverage_rate', 0):.4f}")
        logger.info(f"    - ç»¼åˆè¦†ç›–: {human_coverage.get('overall_coverage', 0):.4f}")
    
    # ç›¸ä¼¼åº¦åˆ†æ
    if "similarity" in ai_results.get("detailed_analysis", {}):
        ai_similarity = ai_results["detailed_analysis"]["similarity"]
        logger.info("")
        logger.info("  AIç”Ÿæˆç”¨ä¾‹ä¸äººå·¥ç”¨ä¾‹çš„ç›¸ä¼¼åº¦:")
        logger.info(f"    - é«˜ç›¸ä¼¼åº¦ç”¨ä¾‹æ•°: {ai_similarity.get('high_similarity_count', 0)}")
        logger.info(f"    - ä½ç›¸ä¼¼åº¦ç”¨ä¾‹æ•°: {ai_similarity.get('low_similarity_count', 0)}")
        logger.info(f"    - å¹³å‡æœ€å¤§ç›¸ä¼¼åº¦: {ai_similarity.get('mean_max_similarity', 0):.4f}")
        logger.info(f"    - è¦†ç›–ç‡: {ai_similarity.get('coverage_rate', 0):.4f}")
    
    logger.info("")
    
    # æ”¹è¿›å»ºè®®
    logger.info("ğŸ’¡ ã€æ”¹è¿›å»ºè®®ã€‘")
    
    if ai_overall < human_overall:
        improvement_rate = (human_overall - ai_overall) / human_overall * 100
        logger.info(f"  AIç”Ÿæˆç”¨ä¾‹æ€»ä½“è´¨é‡ä½äºäººå·¥ç”¨ä¾‹ {improvement_rate:.1f}%")
        
        # æ‰¾å‡ºæœ€å¼±çš„ç»´åº¦
        min_score = float('inf')
        min_metric = ""
        for metric_name, metric_key in metrics:
            score = ai_scores.get(metric_key, 0)
            if score < min_score:
                min_score = score
                min_metric = metric_name
        
        if min_score < 0.7:
            logger.info(f"  1. é‡ç‚¹æ”¹è¿›: {min_metric} (å½“å‰åˆ†æ•°: {min_score:.4f})")
        
        if ai_scores.get("uniqueness_score", 1) < 0.8:
            logger.info("  2. å¢åŠ ç”¨ä¾‹å¤šæ ·æ€§ï¼Œå‡å°‘é‡å¤")
        
        if ai_scores.get("coverage_score", 1) < 0.8:
            logger.info("  3. æ‰©å¤§éœ€æ±‚è¦†ç›–èŒƒå›´")
        
        if ai_scores.get("avg_quality_score", 1) < 0.8:
            logger.info("  4. æé«˜ç”¨ä¾‹æè¿°çš„æ¸…æ™°åº¦å’Œå…·ä½“æ€§")
    
    else:
        improvement_rate = (ai_overall - human_overall) / human_overall * 100
        logger.info(f"  AIç”Ÿæˆç”¨ä¾‹æ€»ä½“è´¨é‡é«˜äºäººå·¥ç”¨ä¾‹ {improvement_rate:.1f}%")
        logger.info("  âœ“ AIç”Ÿæˆæ•ˆæœè‰¯å¥½ï¼Œå¯ç»§ç»­ä¼˜åŒ–")
    
    logger.info("")


if __name__ == "__main__":
    run_demo()

